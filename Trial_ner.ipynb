{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "biobert_ner.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "STeCPrW4KFpJ",
        "DXPpUMx0fZOh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofiagpmg/Detecting-Cyberbully/blob/main/Trial_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icQKBKPTadOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea5aa586-3a49-41cb-ece1-a9777da1d487"
      },
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 8.9MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/6e/a1365b04d7235f4f1fd55d83fe478d2f96cd96b7c7f0241f64340d884b28/boto3-1.16.8-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 29.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Collecting botocore<1.20.0,>=1.19.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/47/75207416a2b9c116a2975643aa99d5cadd084ddb926db0da3dad3730b8b2/botocore-1.19.8-py2.py3-none-any.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 26.7MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.8->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.8->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.19.8 has requirement urllib3<1.26,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.16.8 botocore-1.19.8 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 39.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 55.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=c390e393a6e0714567c79cfac84ba0853165c49638271f2759502b8ac61c7d35\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.2 transformers-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Gn2SVI7utX"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import itertools\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange\n",
        "from collections import defaultdict, OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig\n",
        "from transformers import BertForTokenClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBIt5HQ33ioB"
      },
      "source": [
        "import pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOeVnOOSfJec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85eed8d7-17cb-4ea2-ab52-d2f40669fdaf"
      },
      "source": [
        "# Get GPU device name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name == '/device:GPU:0':\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "  raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soCBkruYfPUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24e9664-36c7-4dfb-94f6-1ead71cb88ee"
      },
      "source": [
        "# tell Pytorch to use the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "print('We will use the GPU:', torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STeCPrW4KFpJ"
      },
      "source": [
        "# Import BioBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9pgQv8Iffq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047e6da0-ebda-4b17-a0aa-d62348fa44fd"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\" -O biobert_weights && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-30 09:19:07--  https://docs.google.com/uc?export=download&confirm=SHBa&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.69.101, 173.194.69.113, 173.194.69.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.69.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-b4-docs.googleusercontent.com/docs/securesc/a49h21l6skdv9f155bhs1f94lfg7nnls/ggj2d2kb4sf7lesqga30pgc3spqom2lt/1604049525000/13799006341648886493/09688102489001817463Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download [following]\n",
            "--2020-10-30 09:19:07--  https://doc-08-b4-docs.googleusercontent.com/docs/securesc/a49h21l6skdv9f155bhs1f94lfg7nnls/ggj2d2kb4sf7lesqga30pgc3spqom2lt/1604049525000/13799006341648886493/09688102489001817463Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download\n",
            "Resolving doc-08-b4-docs.googleusercontent.com (doc-08-b4-docs.googleusercontent.com)... 172.217.218.132, 2a00:1450:4013:c08::84\n",
            "Connecting to doc-08-b4-docs.googleusercontent.com (doc-08-b4-docs.googleusercontent.com)|172.217.218.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=ue0t7g6d9d7o2&continue=https://doc-08-b4-docs.googleusercontent.com/docs/securesc/a49h21l6skdv9f155bhs1f94lfg7nnls/ggj2d2kb4sf7lesqga30pgc3spqom2lt/1604049525000/13799006341648886493/09688102489001817463Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e%3Ddownload&hash=ogqnod52bn0v2coks9d1f1arkom7btqa [following]\n",
            "--2020-10-30 09:19:07--  https://docs.google.com/nonceSigner?nonce=ue0t7g6d9d7o2&continue=https://doc-08-b4-docs.googleusercontent.com/docs/securesc/a49h21l6skdv9f155bhs1f94lfg7nnls/ggj2d2kb4sf7lesqga30pgc3spqom2lt/1604049525000/13799006341648886493/09688102489001817463Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e%3Ddownload&hash=ogqnod52bn0v2coks9d1f1arkom7btqa\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.69.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-08-b4-docs.googleusercontent.com/docs/securesc/a49h21l6skdv9f155bhs1f94lfg7nnls/ggj2d2kb4sf7lesqga30pgc3spqom2lt/1604049525000/13799006341648886493/09688102489001817463Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download&nonce=ue0t7g6d9d7o2&user=09688102489001817463Z&hash=ps8mg12t47iij824aj0ruugfnc04s5he [following]\n",
            "--2020-10-30 09:19:07--  https://doc-08-b4-docs.googleusercontent.com/docs/securesc/a49h21l6skdv9f155bhs1f94lfg7nnls/ggj2d2kb4sf7lesqga30pgc3spqom2lt/1604049525000/13799006341648886493/09688102489001817463Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download&nonce=ue0t7g6d9d7o2&user=09688102489001817463Z&hash=ps8mg12t47iij824aj0ruugfnc04s5he\n",
            "Connecting to doc-08-b4-docs.googleusercontent.com (doc-08-b4-docs.googleusercontent.com)|172.217.218.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘biobert_weights’\n",
            "\n",
            "biobert_weights         [                <=> ] 382.81M  50.8MB/s    in 7.5s    \n",
            "\n",
            "2020-10-30 09:19:16 (50.8 MB/s) - ‘biobert_weights’ saved [401403346]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDpKJWMBEa1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25822dd8-2048-487c-9fd6-fded37602c5d"
      },
      "source": [
        "!tar -xzf biobert_weights\n",
        "!ls biobert_v1.1_pubmed/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\t\t\tmodel.ckpt-1000000.index  vocab.txt\n",
            "model.ckpt-1000000.data-00000-of-00001\tmodel.ckpt-1000000.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5dhA43zEfrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fb4658-3a0d-4851-9cf0-6522f032eed5"
      },
      "source": [
        "!transformers-cli convert --model_type bert --tf_checkpoint biobert_v1.1_pubmed/model.ckpt-1000000 --config biobert_v1.1_pubmed/bert_config.json --pytorch_dump_output biobert_v1.1_pubmed/pytorch_model.bin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-30 09:19:22.685469: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Converting TensorFlow checkpoint from /content/biobert_v1.1_pubmed/model.ckpt-1000000\n",
            "Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
            "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "Loading TF weight bert/embeddings/word_embeddings with shape [28996, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/pooler/dense/bias with shape [768]\n",
            "Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "Save PyTorch model to biobert_v1.1_pubmed/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vqdJUV1EiEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e43033e-9d0b-4497-bd61-3f6a58943a7f"
      },
      "source": [
        "!ls biobert_v1.1_pubmed/\n",
        "!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json\n",
        "!ls biobert_v1.1_pubmed/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\t\t\tmodel.ckpt-1000000.meta\n",
            "model.ckpt-1000000.data-00000-of-00001\tpytorch_model.bin\n",
            "model.ckpt-1000000.index\t\tvocab.txt\n",
            "config.json\t\t\t\tmodel.ckpt-1000000.meta\n",
            "model.ckpt-1000000.data-00000-of-00001\tpytorch_model.bin\n",
            "model.ckpt-1000000.index\t\tvocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9-3E8ZPLA8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99bf6ed6-d479-482e-a128-2d3000b76650"
      },
      "source": [
        "!ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "biobert_v1.1_pubmed  biobert_weights  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXPpUMx0fZOh"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cSxrv3pKBx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d29b6663-dc3a-4857-8576-db792ff45bfc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPfP8ssmdOeW"
      },
      "source": [
        "MAX_LEN = 75\n",
        "BATCH_SIZE = 32\n",
        "tokenizer = BertTokenizer(vocab_file='biobert_v1.1_pubmed/vocab.txt', do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6aKboidi1NQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6933fbe-8c26-4e10-e6b7-a9d96636d7a3"
      },
      "source": [
        "data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Data/bio_ner/bionlp_tags.csv')\n",
        "tag_values = data['tags'].values\n",
        "vocab_len = len(tag_values)\n",
        "print('Entity Types:',vocab_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entity Types: 74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIVLtuLMmIC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf98d47-f4aa-47d8-eb91-7ce477990218"
      },
      "source": [
        "df_tags = pd.DataFrame({'tags':tag_values})\n",
        "df_tags.to_csv('bionlp_tags.csv',index=False)\n",
        "df = pd.read_csv('bionlp_tags.csv')\n",
        "print('Tag Preview:\\n', df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tag Preview:\n",
            "                       tags\n",
            "0     I-Cellular_component\n",
            "1   E-Gene_or_gene_product\n",
            "2   I-Organism_subdivision\n",
            "3     I-Organism_substance\n",
            "4   B-Gene_or_gene_product\n",
            "..                     ...\n",
            "69                 I-Organ\n",
            "70                S-Cancer\n",
            "71            B-Amino_acid\n",
            "72     S-Anatomical_system\n",
            "73                     PAD\n",
            "\n",
            "[74 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcwhPzqRtpEk"
      },
      "source": [
        "class SentenceFetch(object):\n",
        "  \n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "    self.sentences = []\n",
        "    self.tags = []\n",
        "    self.sent = []\n",
        "    self.tag = []\n",
        "    \n",
        "    # make tsv file readable\n",
        "    with open(self.data) as tsv_f:\n",
        "      reader = csv.reader(tsv_f, delimiter='\\t')\n",
        "      for row in reader:\n",
        "        if len(row) == 0:\n",
        "          if len(self.sent) != len(self.tag):\n",
        "            break\n",
        "          self.sentences.append(self.sent)\n",
        "          self.tags.append(self.tag)\n",
        "          self.sent = []\n",
        "          self.tag = []\n",
        "        else:\n",
        "          self.sent.append(row[0])\n",
        "          self.tag.append(row[1])   \n",
        "\n",
        "  def getSentences(self):\n",
        "    return self.sentences\n",
        "  \n",
        "  def getTags(self):\n",
        "    return self.tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24fTBYV4wNwt"
      },
      "source": [
        "corpora = '/content/gdrive/My Drive/Colab Notebooks/Data/bionlp_corpora'\n",
        "sentences = []\n",
        "tags = []\n",
        "for subdir, dirs, files in os.walk(corpora):\n",
        "    for file in files:\n",
        "        if file == 'train.tsv':\n",
        "            path = os.path.join(subdir, file)\n",
        "            sent = SentenceFetch(path).getSentences()\n",
        "            tag = SentenceFetch(path).getTags()\n",
        "            sentences.extend(sent)\n",
        "            tags.extend(tag)\n",
        "            \n",
        "sentences = sentences[0:20000]\n",
        "tags = tags[0:20000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Oikx9K-B2f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ed5588-04d2-41c4-9288-ea056fc18f2f"
      },
      "source": [
        "print('Sentence Preview:\\n',sentences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence Preview:\n",
            " ['The', 'Cdc6', 'protein', 'is', 'ubiquitinated', 'in', 'vivo', 'for', 'proteolysis', 'in', 'Saccharomyces', 'cerevisiae', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDczy6Tyw-dw"
      },
      "source": [
        "def tok_with_labels(sent, text_labels):\n",
        "  '''tokenize and keep labels intact'''\n",
        "  tok_sent = []\n",
        "  labels = []\n",
        "  for word, label in zip(sent, text_labels):\n",
        "    tok_word = tokenizer.tokenize(word)\n",
        "    n_subwords = len(tok_word)\n",
        "\n",
        "    tok_sent.extend(tok_word)\n",
        "    labels.extend([label] * n_subwords)\n",
        "  return tok_sent, labels\n",
        "\n",
        "tok_texts_and_labels = [tok_with_labels(sent, labs) for sent, labs in zip(sentences, tags)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH1VCcKpyDaL"
      },
      "source": [
        "tok_texts = [tok_label_pair[0] for tok_label_pair in tok_texts_and_labels]\n",
        "labels = [tok_label_pair[1] for tok_label_pair in tok_texts_and_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHPtXTIByG7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "608f9090-72be-4aaf-a50e-3dabca8d16fd"
      },
      "source": [
        "len(tok_texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pZrjN1uyNVl"
      },
      "source": [
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tok_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSIuqkC1yQHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3767467-0871-42c1-a5e4-bf070f91258a"
      },
      "source": [
        "for char in tok_texts:\n",
        "    print('WordPiece Tokenizer Preview:\\n', char)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WordPiece Tokenizer Preview:\n",
            " ['The', 'C', '##d', '##c', '##6', 'protein', 'is', 'u', '##bi', '##qui', '##tina', '##ted', 'in', 'v', '##ivo', 'for', 'pro', '##te', '##oly', '##sis', 'in', 'Sa', '##cc', '##har', '##omy', '##ces', 'c', '##ere', '##vis', '##iae', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIsQlfGaDwJB"
      },
      "source": [
        "tag_values = list(set(itertools.chain.from_iterable(tags)))\n",
        "tag_values.append(\"PAD\")\n",
        "\n",
        "tag2idx = {t: i for i,t in enumerate(tag_values)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUlloNl1BCOL"
      },
      "source": [
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WegdV9MEBGPa"
      },
      "source": [
        "# attention masks make explicit reference to which tokens are actual words vs padded words\n",
        "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHl9n7J7BJOb"
      },
      "source": [
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "\n",
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "val_tags = torch.tensor(val_tags)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVJvgMr2BL9j"
      },
      "source": [
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGlRTtznfV_L"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyG2ydyOBadA"
      },
      "source": [
        "config = BertConfig.from_json_file('biobert_v1.1_pubmed/config.json')\n",
        "tmp_d = torch.load('biobert_v1.1_pubmed/pytorch_model.bin', map_location=device)\n",
        "state_dict = OrderedDict()\n",
        "\n",
        "for i in list(tmp_d.keys())[:199]:\n",
        "    x = i\n",
        "    if i.find('bert') > -1:\n",
        "        x = '.'.join(i.split('.')[1:])\n",
        "    state_dict[x] = tmp_d[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSFH01a3bSIg"
      },
      "source": [
        "class BioBertNER(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_len, config, state_dict):\n",
        "    super().__init__()\n",
        "    self.bert = BertModel(config)\n",
        "    self.bert.load_state_dict(state_dict, strict=False)\n",
        "    self.dropout = nn.Dropout(p=0.3)\n",
        "    self.output = nn.Linear(self.bert.config.hidden_size, vocab_len)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    encoded_layer, _ = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    encl = encoded_layer[-1]\n",
        "    out = self.dropout(encl)\n",
        "    out = self.output(out)\n",
        "    return out, out.argmax(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRFqxzw0hH5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c99a1d-6576-4285-813f-836f94533327"
      },
      "source": [
        "model = BioBertNER(vocab_len,config,state_dict)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BioBertNER(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (output): Linear(in_features=768, out_features=74, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lF1ZFbWqNUm"
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=3e-5,\n",
        "    eps=1e-8\n",
        ")\n",
        "epochs = 3\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmvcc45KPunQ"
      },
      "source": [
        "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    for step,batch in enumerate(data_loader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        outputs,y_hat = model(b_input_ids,b_input_mask)\n",
        "        \n",
        "        _,preds = torch.max(outputs,dim=2)\n",
        "        outputs = outputs.view(-1,outputs.shape[-1])\n",
        "        b_labels_shaped = b_labels.view(-1)\n",
        "        loss = loss_fn(outputs,b_labels_shaped)\n",
        "        correct_predictions += torch.sum(preds == b_labels)\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "    return correct_predictions.double()/len(data_loader) , np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3XQ10okPyIa"
      },
      "source": [
        "def model_eval(model,data_loader,loss_fn,device):\n",
        "    model = model.eval()\n",
        "    \n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(data_loader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "            outputs,y_hat = model(b_input_ids,b_input_mask)\n",
        "        \n",
        "            _,preds = torch.max(outputs,dim=2)\n",
        "            outputs = outputs.view(-1,outputs.shape[-1])\n",
        "            b_labels_shaped = b_labels.view(-1)\n",
        "            loss = loss_fn(outputs,b_labels_shaped)\n",
        "            correct_predictions += torch.sum(preds == b_labels)\n",
        "            losses.append(loss.item())\n",
        "        \n",
        "    \n",
        "    return correct_predictions.double()/len(data_loader) , np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6_FdiWSQWiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8731010a-308d-492e-8a3d-94d55cb908b0"
      },
      "source": [
        "%%time\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "normalizer = BATCH_SIZE*MAX_LEN\n",
        "loss_values = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    total_loss = 0\n",
        "    print(f'======== Epoch {epoch+1}/{epochs} ========')\n",
        "    train_acc,train_loss = train_epoch(model,train_dataloader,loss_fn,optimizer,device,scheduler)\n",
        "    train_acc = train_acc/normalizer\n",
        "    print(f'Train Loss: {train_loss} Train Accuracy: {train_acc}')\n",
        "    total_loss += train_loss.item()\n",
        "    \n",
        "    avg_train_loss = total_loss / len(train_dataloader)  \n",
        "    loss_values.append(avg_train_loss)\n",
        "    \n",
        "    val_acc,val_loss = model_eval(model,valid_dataloader,loss_fn,device)\n",
        "    val_acc = val_acc/normalizer\n",
        "    print(f'Val Loss: {val_loss} Val Accuracy: {val_acc}')\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    \n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Epoch 1/3 ========\n",
            "Train Loss: 0.20507348560418795 Train Accuracy: 0.9524637359384253\n",
            "Val Loss: 0.1048296152481011 Val Accuracy: 0.9566269841269842\n",
            "======== Epoch 2/3 ========\n",
            "Train Loss: 0.09420070371741088 Train Accuracy: 0.9647550325636471\n",
            "Val Loss: 0.09104856618103527 Val Accuracy: 0.9574537037037036\n",
            "======== Epoch 3/3 ========\n",
            "Train Loss: 0.08270481394249847 Train Accuracy: 0.9657519242155124\n",
            "Val Loss: 0.08849428369412346 Val Accuracy: 0.958617724867725\n",
            "CPU times: user 6min, sys: 2min 59s, total: 8min 59s\n",
            "Wall time: 9min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyow1_IfFEB9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "efbfdbb5-afc8-47cc-dc99-e4bce28d8bd6"
      },
      "source": [
        "sns.set(style='darkgrid')\n",
        "\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# learning curve\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZO9phN3eivG"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZQuYREzd_EV"
      },
      "source": [
        "text = \"\"\"In addition to their essential catalytic role in protein biosynthesis, aminoacyl-tRNA synthetases participate in numerous other functions, including regulation of gene expression and amino acid biosynthesis via transamidation pathways. Herein, we describe a class of aminoacyl-tRNA synthetase-like (HisZ) proteins based on the catalytic core of the contemporary class II histidyl-tRNA synthetase whose members lack aminoacylation activity but are instead essential components of the first enzyme in histidine biosynthesis ATP phosphoribosyltransferase (HisG). Prediction of the function of HisZ in Lactococcus lactis was assisted by comparative genomics, a technique that revealed a link between the presence or the absence of HisZ and a systematic variation in the length of the HisG polypeptide. HisZ is required for histidine prototrophy, and three other lines of evidence support the direct involvement of HisZ in the transferase function. (i) Genetic experiments demonstrate that complementation of an in-frame deletion of HisG from Escherichia coli (which does not possess HisZ) requires both HisG and HisZ from L. lactis. (ii) Coelution of HisG and HisZ during affinity chromatography provides evidence of direct physical interaction. (iii) Both HisG and HisZ are required for catalysis of the ATP phosphoribosyltransferase reaction. This observation of a common protein domain linking amino acid biosynthesis and protein synthesis implies an early connection between the biosynthesis of amino acids and proteins.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ixSMp8Zd1qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a319970-0105-44bc-d9d2-4da279983f74"
      },
      "source": [
        "nltk.download('punkt')\n",
        "sent_text = nltk.sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQJDBjDOeXzZ"
      },
      "source": [
        "tokenized_text = []\n",
        "for sentence in sent_text:\n",
        "    tokenized_text.append(nltk.word_tokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDysDKAJeaUt"
      },
      "source": [
        "def tokenize_and_preserve(sentence):\n",
        "    tokenized_sentence = []\n",
        "    \n",
        "    for word in sentence:\n",
        "        tokenized_word = tokenizer.tokenize(word)   \n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "    return tokenized_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsCunKjVgC1d"
      },
      "source": [
        "tok_texts = [\n",
        "    tokenize_and_preserve(sent) for sent in tokenized_text\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDapUO6oefFo"
      },
      "source": [
        "input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tok_texts]\n",
        "input_attentions = [[1]*len(in_id) for in_id in input_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS7KQdGqerGo"
      },
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[1])\n",
        "new_tokens, new_labels = [], []\n",
        "for token in tokens:\n",
        "    if token.startswith(\"##\"):\n",
        "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "    else:\n",
        "        \n",
        "        new_tokens.append(token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw23WTdMeuB8"
      },
      "source": [
        "actual_sentences = []\n",
        "pred_labels = []\n",
        "for x,y in zip(input_ids,input_attentions):\n",
        "    x = torch.tensor(x).cuda()\n",
        "    y = torch.tensor(y).cuda()\n",
        "    x = x.view(-1,x.size()[-1])\n",
        "    y = y.view(-1,y.size()[-1])\n",
        "    with torch.no_grad():\n",
        "        _,y_hat = model(x,y)\n",
        "    label_indices = y_hat.to('cpu').numpy()\n",
        "    \n",
        "    tokens = tokenizer.convert_ids_to_tokens(x.to('cpu').numpy()[0])\n",
        "    new_tokens, new_labels = [], []\n",
        "    for token, label_idx in zip(tokens, label_indices[0]):\n",
        "        if token.startswith(\"##\"):\n",
        "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "        else:\n",
        "            new_labels.append(tag_values[label_idx])\n",
        "            new_tokens.append(token)\n",
        "    actual_sentences.append(new_tokens)\n",
        "    pred_labels.append(new_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X12WbCxJYxb8"
      },
      "source": [
        "for token, label in zip(actual_sentences, pred_labels):\n",
        "    for t,l in zip(token,label):\n",
        "        print(\"{}\\t{}\".format(t, l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1f42dmKPhOW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "b88d25d7-ea54-4f59-f856-7d2ee6458717"
      },
      "source": [
        "model_save = 'BIONER_classifier.pt'\n",
        "path = F\"models/{model_save}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-e95d4660a552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'BIONER_classifier.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mF\"models/{model_save}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/BIONER_classifier.pt'"
          ]
        }
      ]
    }
  ]
}